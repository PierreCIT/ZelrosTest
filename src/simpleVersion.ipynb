{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zelros technical test : version simple (no word embedding)\n",
    "Import of necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "dir_path = os.path.dirname(os.path.realpath(\"./src\"))\n",
    "sys.path.insert(0, dir_path)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "from tqdm._tqdm_notebook import tqdm_notebook \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import re\n",
    "import heapq\n",
    "from keras_tqdm_mod.tqdm_notebook_callback import TQDMNotebookCallback\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "Input = tf.keras.layers.Input\n",
    "Bidirectional = tf.keras.layers.Bidirectional\n",
    "CuDNNLSTM = tf.compat.v1.keras.layers.CuDNNLSTM\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "GlobalMaxPool1D = tf.keras.layers.GlobalMaxPool1D\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables to use in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_vocabulary_length = 300 # The maximum number of words in the vocabulary (171 000 words in english dictionary)\n",
    "batch_size = 3000 # Training batch size\n",
    "validation_batch_size = 3000 # Validation batch size\n",
    "epochs = 6 # number of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True), input_shape=(1,max_vocabulary_length)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../dataset/train.csv\")\n",
    "print(\"{} training data available\".format(data_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove ponctuations and unnecessary spaces in sentences as well as transfer to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_lower_case_and_rm_double_spaces_poncuation(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\W',' ',sentence) # Remove punctuation (non-word caracters)\n",
    "    sentence = re.sub(r'\\s+',' ',sentence) # Remove multiple space\n",
    "    if sentence[-1]==\" \": # Remove useless space at the end of the sentence\n",
    "        sentence = sentence[:-1]\n",
    "    return sentence\n",
    "\n",
    "data_df[\"question_text\"] = data_df[\"question_text\"].progress_apply(to_lower_case_and_rm_double_spaces_poncuation)\n",
    "print(\"Check the result on the first sentence : {}\".format(data_df[\"question_text\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset in train data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data_df, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "percentage_in_train = train_df.groupby(\"target\").count()[\"qid\"][1]/train_df.shape[0]\n",
    "percentage_in_val = val_df.groupby(\"target\").count()[\"qid\"][1]/val_df.shape[0]\n",
    "print(f\"Train dataset size: {train_df.shape[0]}, validation size: {val_df.shape[0]}, \"\n",
    "      f\"{math.floor(val_df.shape[0]*100/train_df.shape[0])}% of the training dataset size\")\n",
    "print(\"Percentage of positives in train = {:.2f} and in val {:.2f}\".format(percentage_in_train,percentage_in_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ```bag-of-word``` from the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voc = {} # Contain every word with their number of occurrences\n",
    "for index, row in tqdm_notebook(train_df.iterrows(),total=train_df.shape[0]):\n",
    "    question = row[\"question_text\"]\n",
    "    for word in question.split(\" \"):\n",
    "        if word not in voc.keys():\n",
    "            voc[word] = 1\n",
    "        else:\n",
    "            voc[word] += 1\n",
    "\n",
    "print(\"The vocabulary contains {} words\".format(len(voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reduce the size of the vocabulary to match the maximum value pre-defined.\n",
    "We will keep the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "voc_most_freq = heapq.nlargest(max_vocabulary_length, voc, key=voc.get)\n",
    "for i in range(50):\n",
    "    print(voc_most_freq[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that will vectorize a question using the vocabulary of the most frequently used words. </br>\n",
    "Also define the function that will create the array of vectors from a subset of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_question(question): # We assume the question as already been formatted (lower case, no punctuation, spaces)\n",
    "    vector = np.zeros(max_vocabulary_length) # Initial vector filled with zeros\n",
    "    for _word in question.split(\" \"):\n",
    "        try:\n",
    "            _index = voc_most_freq.index(_word)\n",
    "            vector[_index]+=1\n",
    "        except ValueError:\n",
    "            pass # If the word is not in the vocabulary we do nothing\n",
    "    return vector\n",
    "\n",
    "def create_vectors_from_dataframe(data):\n",
    "    vectors = np.zeros((data.shape[0],1, max_vocabulary_length), dtype=np.int)\n",
    "    i = 0 \n",
    "    for _index, _row in data.iterrows():\n",
    "        vectors[i][0] = vectorize_question(_row[1])\n",
    "        i+=1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training generator to feed data to the network, and a validation data generator to check the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_generator(_train_df):\n",
    "    nb_batches = _train_df.shape[0]//batch_size\n",
    "#     print(\"nb batches : \",nb_batches)\n",
    "    while True:\n",
    "#         print(\"New epoch\")\n",
    "        _train_df = _train_df.sample(frac=1) # shuffle the data\n",
    "        for i in range(nb_batches):\n",
    "            vectors = create_vectors_from_dataframe(_train_df.iloc[i*batch_size:(i+1)*batch_size])\n",
    "            yield (np.asarray(vectors), np.asarray(_train_df[\"target\"][i*batch_size:(i+1)*batch_size].values))\n",
    "\n",
    "def validation_generator(_val_df):\n",
    "    nb_batches = _val_df.shape[0]//validation_batch_size\n",
    "    \n",
    "    while True:\n",
    "        for i in range(nb_batches):\n",
    "            vectors = create_vectors_from_dataframe(_val_df.iloc[i*batch_size:(i+1)*batch_size])\n",
    "            yield (np.asarray(vectors), np.asarray(_val_df[\"target\"][i*batch_size:(i+1)*batch_size].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator = training_generator(train_df)\n",
    "# a, b = generator.__next__()\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "\n",
    "print(\"steps per epoch = {}, epochs = {}, batch_size = {}\".format(train_df.shape[0] // batch_size, epochs, batch_size))\n",
    "model.fit_generator(generator, steps_per_epoch=train_df.shape[0] // batch_size, epochs=epochs, verbose=0,\n",
    "                   callbacks=[TQDMNotebookCallback()])\n",
    "\n",
    "model.evaluate_generator(validation_generator(val_df),val_df.shape[0]//validation_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
