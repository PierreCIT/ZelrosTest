{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zelros technical test : version simple (no word embedding)\n",
    "Import of necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.0.0\n",
      "Num GPUs available:  1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "dir_path = os.path.dirname(os.path.realpath(\"./src\"))\n",
    "sys.path.insert(0, dir_path)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "from tqdm._tqdm_notebook import tqdm_notebook \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import re\n",
    "import heapq\n",
    "from sklearn import metrics\n",
    "from keras_tqdm_mod.tqdm_notebook_callback import TQDMNotebookCallback\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "Input = tf.keras.layers.Input\n",
    "Bidirectional = tf.keras.layers.Bidirectional\n",
    "CuDNNLSTM = tf.compat.v1.keras.layers.CuDNNLSTM\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "GlobalMaxPool1D = tf.keras.layers.GlobalMaxPool1D\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs available: \", len(gpus))\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables to use in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "max_vocabulary_length = 500 # The maximum number of words in the vocabulary (171 000 words in english dictionary)\n",
    "batch_size = 2000 # Training batch size\n",
    "validation_batch_size = 3000 # Validation batch size\n",
    "epochs = 2 # number of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 1, 128)            187392    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1, 128)            99328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 288,801\n",
      "Trainable params: 288,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True), input_shape=(1,max_vocabulary_length)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122 training data available\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"../dataset/train.csv\")\n",
    "print(\"{} training data available\".format(data_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove ponctuations and unnecessary spaces in sentences as well as transfer to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d109c0739b5749b984d3bb2d26a045eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check the result on the first sentence : how did quebec nationalists see their province as a nation in the 1960s\n"
     ]
    }
   ],
   "source": [
    "def to_lower_case_and_rm_double_spaces_poncuation(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\W',' ',sentence) # Remove punctuation (non-word caracters)\n",
    "    sentence = re.sub(r'\\s+',' ',sentence) # Remove multiple space\n",
    "    if sentence[-1]==\" \": # Remove useless space at the end of the sentence\n",
    "        sentence = sentence[:-1]\n",
    "    return sentence\n",
    "\n",
    "data_df[\"question_text\"] = data_df[\"question_text\"].progress_apply(to_lower_case_and_rm_double_spaces_poncuation)\n",
    "print(\"Check the result on the first sentence : {}\".format(data_df[\"question_text\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset in train data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data_df, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1175509, validation size: 130613, 11% of the training dataset size\n",
      "Percentage of positives in train = 0.06 and in val 0.06\n"
     ]
    }
   ],
   "source": [
    "percentage_in_train = train_df.groupby(\"target\").count()[\"qid\"][1]/train_df.shape[0]\n",
    "percentage_in_val = val_df.groupby(\"target\").count()[\"qid\"][1]/val_df.shape[0]\n",
    "print(f\"Train dataset size: {train_df.shape[0]}, validation size: {val_df.shape[0]}, \"\n",
    "      f\"{math.floor(val_df.shape[0]*100/train_df.shape[0])}% of the training dataset size\")\n",
    "print(\"Percentage of positives in train = {:.2f} and in val {:.2f}\".format(percentage_in_train,percentage_in_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ```bag-of-word``` from the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8be1c7de4aa4276a53fd83f3d64820b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1175509), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The vocabulary contains 184950 words\n"
     ]
    }
   ],
   "source": [
    "voc = {} # Contain every word with their number of occurrences\n",
    "for index, row in tqdm_notebook(train_df.iterrows(),total=train_df.shape[0]):\n",
    "    question = row[\"question_text\"]\n",
    "    for word in question.split(\" \"):\n",
    "        if word not in voc.keys():\n",
    "            voc[word] = 1\n",
    "        else:\n",
    "            voc[word] += 1\n",
    "\n",
    "print(\"The vocabulary contains {} words\".format(len(voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reduce the size of the vocabulary to match the maximum value pre-defined.\n",
    "We will keep the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "what\n",
      "is\n",
      "a\n",
      "to\n",
      "in\n",
      "of\n",
      "i\n",
      "how\n",
      "and\n",
      "do\n",
      "are\n",
      "for\n",
      "you\n",
      "can\n",
      "why\n",
      "it\n",
      "my\n",
      "that\n",
      "if\n",
      "with\n",
      "on\n",
      "or\n",
      "have\n",
      "be\n",
      "does\n",
      "s\n",
      "from\n",
      "your\n",
      "an\n",
      "which\n",
      "should\n",
      "when\n",
      "get\n",
      "best\n",
      "would\n",
      "as\n",
      "people\n",
      "t\n",
      "there\n",
      "some\n",
      "who\n",
      "will\n",
      "like\n",
      "not\n",
      "at\n",
      "about\n",
      "they\n",
      "by\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "voc_most_freq = heapq.nlargest(max_vocabulary_length, voc, key=voc.get)\n",
    "for i in range(50):\n",
    "    print(voc_most_freq[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that will vectorize a question using the vocabulary of the most frequently used words. </br>\n",
    "Also define the function that will create the array of vectors from a subset of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_question(question): # We assume the question as already been formatted (lower case, no punctuation, spaces)\n",
    "    vector = np.zeros(max_vocabulary_length) # Initial vector filled with zeros\n",
    "    for _word in question.split(\" \"):\n",
    "        try:\n",
    "            _index = voc_most_freq.index(_word)\n",
    "            vector[_index]+=1\n",
    "        except ValueError:\n",
    "            pass # If the word is not in the vocabulary we do nothing\n",
    "    return vector\n",
    "\n",
    "def create_vectors_from_dataframe(data):\n",
    "    vectors = np.zeros((data.shape[0],1, max_vocabulary_length), dtype=np.int)\n",
    "    i = 0 \n",
    "    for _index, _row in data.iterrows():\n",
    "        vectors[i][0] = vectorize_question(_row[1])\n",
    "        i+=1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training generator to feed data to the network, and a validation data generator to check the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_generator(_train_df):\n",
    "    nb_batches = _train_df.shape[0]//batch_size\n",
    "#     print(\"nb batches : \",nb_batches)\n",
    "    while True:\n",
    "#         print(\"New epoch\")\n",
    "        _train_df = _train_df.sample(frac=1) # shuffle the data\n",
    "        for i in range(nb_batches):\n",
    "            vectors = create_vectors_from_dataframe(_train_df.iloc[i*batch_size:(i+1)*batch_size])\n",
    "            yield (np.asarray(vectors), np.asarray(_train_df[\"target\"][i*batch_size:(i+1)*batch_size].values))\n",
    "\n",
    "def validation_generator(_val_df, predict=False):\n",
    "    nb_batches = _val_df.shape[0]//validation_batch_size\n",
    "    \n",
    "    while True:\n",
    "        for i in range(nb_batches):\n",
    "            vectors = create_vectors_from_dataframe(_val_df.iloc[i*batch_size:(i+1)*batch_size])\n",
    "            if not predict:\n",
    "                yield (np.asarray(vectors), np.asarray(_val_df[\"target\"][i*batch_size:(i+1)*batch_size].values))\n",
    "            else:\n",
    "                yield np.asarray(vectors)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch = 783, epochs = 1, batch_size = 1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf13adc0e654bb8ac6de0b270410ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=1, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5cf0450d094a278a80e304fc24d6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', style=ProgressStyle(description_width='initial')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "On the validation dataset the loss is 0.171 and accuracy is 0.940\n"
     ]
    }
   ],
   "source": [
    "generator = training_generator(train_df)\n",
    "# a, b = generator.__next__()\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "\n",
    "print(\"steps per epoch = {}, epochs = {}, batch_size = {}\".format(train_df.shape[0] // batch_size, epochs, batch_size))\n",
    "model.fit_generator(generator, steps_per_epoch=100, epochs=epochs, verbose=0,\n",
    "                   callbacks=[TQDMNotebookCallback()])\n",
    "\n",
    "results = model.evaluate_generator(validation_generator(val_df),val_df.shape[0]//validation_batch_size)\n",
    "print(\"On the validation dataset the loss is {:.3f} and accuracy is {:.3f}\".format(results[0], results[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compute the predictions for all validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at the threshold 0.01 is 0.14816535498917519\n",
      "F1 score at the threshold 0.02 is 0.19299128946175922\n",
      "F1 score at the threshold 0.03 is 0.22976014903361022\n",
      "F1 score at the threshold 0.04 is 0.25722408147108566\n",
      "F1 score at the threshold 0.05 is 0.2806646893917848\n",
      "F1 score at the threshold 0.06 is 0.30303167934597475\n",
      "F1 score at the threshold 0.07 is 0.3210587738683485\n",
      "F1 score at the threshold 0.08 is 0.33565798263193053\n",
      "F1 score at the threshold 0.09 is 0.3494334439739041\n",
      "F1 score at the threshold 0.1 is 0.3627145562274754\n",
      "F1 score at the threshold 0.11 is 0.3743832707074008\n",
      "F1 score at the threshold 0.12 is 0.383871744590085\n",
      "F1 score at the threshold 0.13 is 0.39282998370450845\n",
      "F1 score at the threshold 0.14 is 0.40066702038960056\n",
      "F1 score at the threshold 0.15 is 0.4064365634760606\n",
      "F1 score at the threshold 0.16 is 0.4119948922848787\n",
      "F1 score at the threshold 0.17 is 0.4179411386711219\n",
      "F1 score at the threshold 0.18 is 0.4217741935483871\n",
      "F1 score at the threshold 0.19 is 0.42402006316180574\n",
      "F1 score at the threshold 0.2 is 0.42365715383876235\n",
      "F1 score at the threshold 0.21 is 0.42413690180081587\n",
      "F1 score at the threshold 0.22 is 0.4245094523285005\n",
      "F1 score at the threshold 0.23 is 0.4249208025343189\n",
      "F1 score at the threshold 0.24 is 0.4253305043251183\n",
      "F1 score at the threshold 0.25 is 0.42420168067226893\n",
      "F1 score at the threshold 0.26 is 0.42199194012665514\n",
      "F1 score at the threshold 0.27 is 0.42151988636363635\n",
      "F1 score at the threshold 0.28 is 0.42025901380190916\n",
      "F1 score at the threshold 0.29 is 0.4177522907186935\n",
      "F1 score at the threshold 0.3 is 0.41416858826543684\n",
      "F1 score at the threshold 0.31 is 0.41167997904936493\n",
      "F1 score at the threshold 0.32 is 0.4075835733905005\n",
      "F1 score at the threshold 0.33 is 0.40271772699197034\n",
      "F1 score at the threshold 0.34 is 0.397700988294666\n",
      "F1 score at the threshold 0.35 is 0.3906395807910416\n",
      "F1 score at the threshold 0.36 is 0.3825985059323276\n",
      "F1 score at the threshold 0.37 is 0.37748788669399924\n",
      "F1 score at the threshold 0.38 is 0.36982450809086076\n",
      "F1 score at the threshold 0.39 is 0.3632848747293535\n",
      "F1 score at the threshold 0.4 is 0.35823709639406076\n",
      "F1 score at the threshold 0.41 is 0.35048745405146237\n",
      "F1 score at the threshold 0.42 is 0.34499553535189537\n",
      "F1 score at the threshold 0.43 is 0.3367237244687009\n",
      "F1 score at the threshold 0.44 is 0.330062893081761\n",
      "F1 score at the threshold 0.45 is 0.3225641897636456\n",
      "F1 score at the threshold 0.46 is 0.31662496764731257\n",
      "F1 score at the threshold 0.47 is 0.3087565674255692\n",
      "F1 score at the threshold 0.48 is 0.3001512320967885\n",
      "F1 score at the threshold 0.49 is 0.2885906040268456\n",
      "F1 score at the threshold 0.5 is 0.2646531382727702\n",
      "Best results for a threshold of 0.24 with F1 score of 0.4253305043251183\n"
     ]
    }
   ],
   "source": [
    "predictions_val = model.predict(np.asarray(create_vectors_from_dataframe(val_df), dtype=np.float16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "Use the F1 score to compute the threshold for insincere questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max=0\n",
    "threshold = 0\n",
    "for thresh_test in np.arange(0.01, 0.51, 0.01):\n",
    "    thresh_test = np.round(thresh_test,2)\n",
    "    F1_score = metrics.f1_score(val_df[\"target\"],(predictions_val>thresh_test).astype(int))\n",
    "    if F1_score>_max: _max,threshold = F1_score, thresh_test\n",
    "    print(\"F1 score at the threshold {} is {}\".format(thresh_test,F1_score))\n",
    "\n",
    "print(\"\\nBest results for a threshold of {} with F1 score of {}\".format(threshold, _max))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
