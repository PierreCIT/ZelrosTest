{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zelros technical test : version simple (no word embedding)\n",
    "Import of necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.0.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "dir_path = os.path.dirname(os.path.realpath(\"./src\"))\n",
    "sys.path.insert(0, dir_path)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "from tqdm._tqdm_notebook import tqdm_notebook \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import re\n",
    "import heapq\n",
    "from keras_tqdm_mod.tqdm_notebook_callback import TQDMNotebookCallback\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "Input = tf.keras.layers.Input\n",
    "Bidirectional = tf.keras.layers.Bidirectional\n",
    "CuDNNLSTM = tf.compat.v1.keras.layers.CuDNNLSTM\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "GlobalMaxPool1D = tf.keras.layers.GlobalMaxPool1D\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables to use in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_vocabulary_length = 300 # The maximum number of words in the vocabulary (171 000 words in english dictionary)\n",
    "batch_size = 3000 # Training batch size\n",
    "validation_batch_size = 3000 # Validation batch size\n",
    "epochs = 6 # number of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 1, 128)            187392    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1, 128)            99328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 288,801\n",
      "Trainable params: 288,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True), input_shape=(1,max_vocabulary_length)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122 training data available\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"../dataset/train.csv\")\n",
    "print(\"{} training data available\".format(data_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove ponctuations and unnecessary spaces in sentences as well as transfer to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2077875128411486cfb066cd1b30a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check the result on the first sentence : how did quebec nationalists see their province as a nation in the 1960s\n"
     ]
    }
   ],
   "source": [
    "def to_lower_case_and_rm_double_spaces_poncuation(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\W',' ',sentence) # Remove punctuation (non-word caracters)\n",
    "    sentence = re.sub(r'\\s+',' ',sentence) # Remove multiple space\n",
    "    if sentence[-1]==\" \": # Remove useless space at the end of the sentence\n",
    "        sentence = sentence[:-1]\n",
    "    return sentence\n",
    "\n",
    "data_df[\"question_text\"] = data_df[\"question_text\"].progress_apply(to_lower_case_and_rm_double_spaces_poncuation)\n",
    "print(\"Check the result on the first sentence : {}\".format(data_df[\"question_text\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset in train data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data_df, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1175509, validation size: 130613, 11% of the training dataset size\n",
      "Percentage of positives in train = 0.06 and in val 0.06\n"
     ]
    }
   ],
   "source": [
    "percentage_in_train = train_df.groupby(\"target\").count()[\"qid\"][1]/train_df.shape[0]\n",
    "percentage_in_val = val_df.groupby(\"target\").count()[\"qid\"][1]/val_df.shape[0]\n",
    "print(f\"Train dataset size: {train_df.shape[0]}, validation size: {val_df.shape[0]}, \"\n",
    "      f\"{math.floor(val_df.shape[0]*100/train_df.shape[0])}% of the training dataset size\")\n",
    "print(\"Percentage of positives in train = {:.2f} and in val {:.2f}\".format(percentage_in_train,percentage_in_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ```bag-of-word``` from the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4bbfe15c9646c9b19f253605e47db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1175509), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The vocabulary contains 184606 words\n"
     ]
    }
   ],
   "source": [
    "voc = {} # Contain every word with their number of occurrences\n",
    "for index, row in tqdm_notebook(train_df.iterrows(),total=train_df.shape[0]):\n",
    "    question = row[\"question_text\"]\n",
    "    for word in question.split(\" \"):\n",
    "        if word not in voc.keys():\n",
    "            voc[word] = 1\n",
    "        else:\n",
    "            voc[word] += 1\n",
    "\n",
    "print(\"The vocabulary contains {} words\".format(len(voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reduce the size of the vocabulary to match the maximum value pre-defined.\n",
    "We will keep the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "what\n",
      "is\n",
      "a\n",
      "to\n",
      "in\n",
      "of\n",
      "i\n",
      "how\n",
      "and\n",
      "do\n",
      "are\n",
      "for\n",
      "you\n",
      "can\n",
      "why\n",
      "it\n",
      "my\n",
      "that\n",
      "if\n",
      "with\n",
      "on\n",
      "or\n",
      "have\n",
      "be\n",
      "does\n",
      "s\n",
      "from\n",
      "your\n",
      "an\n",
      "which\n",
      "should\n",
      "when\n",
      "get\n",
      "best\n",
      "would\n",
      "as\n",
      "people\n",
      "t\n",
      "some\n",
      "there\n",
      "who\n",
      "will\n",
      "like\n",
      "at\n",
      "not\n",
      "about\n",
      "they\n",
      "by\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "voc_most_freq = heapq.nlargest(max_vocabulary_length, voc, key=voc.get)\n",
    "for i in range(50):\n",
    "    print(voc_most_freq[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that will vectorize a question using the vocabulary of the most frequently used words. </br>\n",
    "Also define the function that will create the array of vectors from a subset of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_question(question): # We assume the question as already been formatted (lower case, no punctuation, spaces)\n",
    "    vector = np.zeros(max_vocabulary_length) # Initial vector filled with zeros\n",
    "    for _word in question.split(\" \"):\n",
    "        try:\n",
    "            _index = voc_most_freq.index(_word)\n",
    "            vector[_index]+=1\n",
    "        except ValueError:\n",
    "            pass # If the word is not in the vocabulary we do nothing\n",
    "    return vector\n",
    "\n",
    "def create_vectors_from_dataframe(data):\n",
    "    vectors = np.zeros((data.shape[0],1, max_vocabulary_length), dtype=np.int)\n",
    "    i = 0 \n",
    "    for _index, _row in data.iterrows():\n",
    "        vectors[i][0] = vectorize_question(_row[1])\n",
    "        i+=1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training generator to feed data to the network, and a validation data generator to check the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_generator(_train_df):\n",
    "    nb_batches = _train_df.shape[0]//batch_size\n",
    "#     print(\"nb batches : \",nb_batches)\n",
    "    while True:\n",
    "#         print(\"New epoch\")\n",
    "        _train_df = _train_df.sample(frac=1) # shuffle the data\n",
    "        for i in range(nb_batches):\n",
    "            vectors = create_vectors_from_dataframe(_train_df.iloc[i*batch_size:(i+1)*batch_size])\n",
    "            yield (np.asarray(vectors), np.asarray(_train_df[\"target\"][i*batch_size:(i+1)*batch_size].values))\n",
    "\n",
    "def validation_generator(_val_df):\n",
    "    nb_batches = _val_df.shape[0]//validation_batch_size\n",
    "    \n",
    "    while True:\n",
    "        for i in range(nb_batches):\n",
    "            vectors = create_vectors_from_dataframe(_val_df.iloc[i*batch_size:(i+1)*batch_size])\n",
    "            yield (np.asarray(vectors), np.asarray(_val_df[\"target\"][i*batch_size:(i+1)*batch_size].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch = 391, epochs = 6, batch_size = 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d98ad9221b4b479cd3483c78e64275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=6, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fddd88e7214792a0634d286706915e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=391, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfbd19814804eadba25865977ca04f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=391, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5733dc316faf4ac0b14c41653274d31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=391, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49b4237ac594a9394d39cb0c31fe0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=391, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51473944e0a9425abf06ff73427ee0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=391, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c2e160698a433f9f470e6732fd0d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=391, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14958929530409878, 0.9456744]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = training_generator(train_df)\n",
    "# a, b = generator.__next__()\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "\n",
    "print(\"steps per epoch = {}, epochs = {}, batch_size = {}\".format(train_df.shape[0] // batch_size, epochs, batch_size))\n",
    "model.fit_generator(generator, steps_per_epoch=train_df.shape[0] // batch_size, epochs=epochs, verbose=0,\n",
    "                   callbacks=[TQDMNotebookCallback()])\n",
    "\n",
    "model.evaluate_generator(validation_generator(val_df),val_df.shape[0]//validation_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
